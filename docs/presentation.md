# Industrial Dual-Context RAG ‚Äî Slide Deck

> **Tempo alvo:** 18 minutos + 7 minutos de perguntas. Cada se√ß√£o abaixo corresponde a 1 slide (salvo indica√ß√£o).

## 1. Capa e Motiva√ß√£o
- T√≠tulo completo, autores, disciplina (IA Generativa) e data da defesa.
- Breve contexto: diagn√≥sticos de torno mec√¢nico exigem correlacionar manuais PDF e telemetria MQTT em tempo quase real.
- Destaque visual: foto/√≠cone de m√°quina + fluxo "Manual + Sensores ‚Üí LLM".

## 2. Problema e Objetivos (2 slides)
1. **Problema**: operadores dependem de conhecimento t√°cito; LLMs puros alucinam sem contexto; telemetria isolada n√£o gera recomenda√ß√µes.
2. **Objetivos**:
	 - Comparar tr√™s n√≠veis de contexto (Baseline, RAG Est√°tico, RAG Dual).
	 - Reduzir alucina√ß√µes e prover explicabilidade (chunks citados, telemetria selecionada).
	 - Disponibilizar ferramenta reprodut√≠vel (Docker + notebooks + bot√£o de consolida√ß√£o).

## 3. Arquitetura Macro (2 slides)
- Diagrama Docker Compose mostrando `simulator`, `api`, `web`, `ollama`, `weaviate` e o volume compartilhado `./docs:/app/docs` para gabaritos din√¢micos.
- Fluxo de dados:
	1. Telemetria MQTT (simulador ‚Üí broker ‚Üí Streamlit ‚Üí API).
	2. Diagn√≥stico on-demand (UI ‚Üí API ‚Üí Vector Store/LLM ‚Üí UI).
- Destacar montagem de volumes (`./data/api`, `./data/weaviate`, `./docs`) e reindexa√ß√£o autom√°tica (`‚ôªÔ∏è`).
- Observa√ß√£o: Weaviate roda sem m√≥dulos text2vec; consultas usam `nearVector`, enquanto FAISS permanece embutido na API (sem servi√ßo externo).

## 4. Pipeline RAG Dual (2 slides)
- Slide 1: ingest√£o ‚Üí chunking configur√°vel ‚Üí embeddings Sentence-Transformers ‚Üí upsert no backend escolhido.
- Slide 2: sele√ß√£o de sinais (multiselect), recupera√ß√£o top-k, montagem do prompt com instru√ß√µes customizadas e formato JSON.
- Chamar aten√ß√£o de que o modelo de embedding (Sentence-Transformers, default `all-MiniLM-L6-v2`) √© escolhido uma √∫nica vez na UI e reaproveitado em todos os backends; trocar o backend requer apenas reprocessar os PDFs.
- Nota: logs incluem tokens, contexto utilizado e telemetria realmente inserida no prompt. O backend exp√µe `build_vector_debug`, que serializa vetores (pergunta + chunks) e similaridades cosseno para auditoria.

## 5. Tecnologias e Modelos
- LLMs: Groq (Llama3-8B/70B), Google Gemini 2.5 Flash, Ollama (Llama3.2 3B, offline).
- Vetores: ChromaDB local, FAISS, Weaviate dockerizado, Pinecone serverless.
- Outras libs: FastAPI, Streamlit, LangChain, MQTT (paho), Plotly para relat√≥rios.

## 6. Metodologia Experimental (2 slides)
1. **Cen√°rios avaliados**: Baseline, RAG Est√°tico, RAG Dual; cada um executado com estado normal e falhas (superaquecimento, desbalanceamento).
2. **Procedimento**:
	 - Upload do manual de 45 p√°ginas.
	 - Ajuste dos sinais de telemetria e chunking.
	 - Inje√ß√£o de falhas via bot√µes.
	 - Coleta autom√°tica de m√©tricas (accuracy, BLEU, ROUGE-L, **BERTScore F1**, lat√™ncia, tokens) + gabarito autom√°tico (JSONs em `docs/gabaritos.json`, carregados ao clicar nos bot√µes do simulador).
	 - Bot√£o "üìä Gerar resumo autom√°tico" produz CSV/HTML em `data/api/summaries`.

## 7. Resultados e Insights (2 slides)
- Slide 1 (Tabela/Gr√°fico): apresentar m√©dias ‚Üí Baseline (acc 0.41), RAG Est√°tico (0.68), RAG Dual (0.89).
- Slide 2 (Hist√≥rias):
	- Caso "PE√áA SOLTA": Dual cita limite ISO 10816 e recomenda parada; Baseline descreve genericamente vibra√ß√£o.
	- Ablation: remover sinal de vibra√ß√£o reduz acur√°cia para 0.74, provando import√¢ncia do seletor de sensores.
- Complementar: monitoramento de tokens mostrou m√©dia de 1.9k tokens/prompt e 0.8k tokens/resposta no cen√°rio Dual, auxiliando na estimativa de custos.

- Passos numerados: 1) subir Docker, 2) indexar PDF, 3) selecionar sinais e backend, 4) injetar falha, 5) comparar cen√°rios e exportar relat√≥rio.
- Destacar que os gabaritos s√£o preenchidos automaticamente ao clicar nos bot√µes do simulador (sem copiar/colar JSON manualmente).
- Screenshots da UI (sidebar + painel de diagn√≥stico + bot√£o de resumo).

## 9. Limita√ß√µes & Pr√≥ximos Passos
- Broker MQTT sem SLA ‚Üí migrar para broker com QoS.
- Rate limit das APIs Groq/Gemini ‚Üí manter fallback local e implementar fila de requisi√ß√µes.
- Cada backend vetorial requer reindex = pretende-se sincronizar automaticamente.
- Expandir sensores (rota√ß√£o, press√£o) e adicionar aprendizado ativo.

## 10. Conclus√µes
- Contexto dual reduz alucina√ß√µes e aumenta rastreabilidade.
- Ferramenta dockerizada facilita reprodu√ß√£o acad√™mica e PoCs industriais.
- Pr√≥ximas etapas: integrar modelos especializados e publicar dataset/logs.

## 11. Refer√™ncias
- Normas ISO 10816 / ISO 20816.
- Documenta√ß√£o Groq, Google AI Studio, Ollama.
- Trabalhos correlatos de RAG industrial (citar papers selecionados).


## OBS

Explaining build_vector_debug in api/main.py
No backend da API, tudo isso est√° concentrado em main.py.

A fun√ß√£o build_vector_debug fica por volta da linha 260 do arquivo. Ela recebe a pergunta, os chunks retornados pelo backend (j√° limitados pelo top_k=3), os metadados e o nome do modelo de embedding. Dentro dela:

Reusa o embedder HuggingFaceEmbeddings para gerar o vetor da pergunta (embed_query) e de cada chunk (embed_documents).
Calcula a similaridade cosseno manualmente (cosine_similarity definido um pouco acima), para cada chunk recuperado.
Monta uma lista chamada retrieved com index, source, similarity, chunk_preview e o vetor completo do chunk (clippado depois na UI).
Retorna um dicion√°rio com embedding_model, metric (sempre cosine), o vetor da pergunta (query_embedding) e a lista retrieved.
A fun√ß√£o √© chamada logo ap√≥s a recupera√ß√£o dos chunks, dentro de run_diagnosis:

Esse vector_debug √© inclu√≠do no payload de resposta e, se o usu√°rio marcar ‚ÄúGerar logs detalhados‚Äù, v√™ o bloco inteiro na interface (com a visualiza√ß√£o slice dos vetores). Se log_experiments estiver ligado, os campos query_embedding e retrieved_vectors v√£o para experiment_logs.csv (como JSON), permitindo reconstruir depois quais trechos foram usados.

Como isso funciona especificamente no Chroma:

query_backend (mesmo arquivo) monta um cliente PersistentClient apontando para a pasta data/chromadb. Ao criar ou reabrir a cole√ß√£o industrial_manuals, usamos SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2").
Durante a indexa√ß√£o (upsert_chunks_to_backend), cada chunk recebe um ID e metadados (fonte, chunk_size, etc.) e √© inserido via collection.upsert.
Na consulta (collection.query(query_texts=[question], n_results=3)), o Chroma gera embeddings com a mesma fun√ß√£o e retorna as tr√™s melhores correspond√™ncias com base em cosseno (padr√£o). Ele j√° traz documents e metadatas.
Em seguida build_vector_debug reconta essa mesma hist√≥ria, mas guardando os vetores em JSON para auditoria (algo que o Chroma n√£o retorna direto).
Ent√£o, para explicar em uma apresenta√ß√£o: foque em query_backend ‚Üí Chroma (cosine similarity, top‚Äëk=3) ‚Üí build_vector_debug para registrar ‚Äúcomo‚Äù o chunk foi escolhido (vetores e similaridade). Se quiser ajustar algo (ex.: mudar top‚Äëk, trocar m√©trica, limitar preview), √© nesse combo de fun√ß√µes em main.py que voc√™ mexe.
A m√©trica bert_score_f1 que registramos vem do BERTScore, especificamente da componente F1. Em termos pr√°ticos:

O BERTScore compara candidato e refer√™ncia posi√ß√£o a posi√ß√£o usando embeddings contextualizados. Ele calcula precis√£o (semelhan√ßa dos tokens do candidato em rela√ß√£o √† refer√™ncia) e revoca√ß√£o (qu√£o bem o candidato cobre tokens da refer√™ncia) e depois faz a m√©dia harm√¥nica, resultando no F1.
O valor cru sai em faixa 0‚Äì1. No nosso c√≥digo, multiplicamos por 100, ent√£o o n√∫mero registrado no CSV representa um percentual (por exemplo, 87.3 significa F1 ‚âà 0.873).
Quanto mais pr√≥ximo de 100, maior a similaridade sem√¢ntica entre a resposta do LLM e o gabarito. Valores abaixo de 50 normalmente indicam diferen√ßa sem√¢ntica forte; acima de 80‚Äì85 sugerem que o conte√∫do principal coincide bem.
Portanto, √© uma medida cont√≠nua de 0 a 100% usada para avaliar ‚Äúqu√£o parecido em sentido‚Äù est√° o texto do modelo em rela√ß√£o ao gabarito, indo al√©m da mera coincid√™ncia literal (BLEU/ROUGE).
BLEU (Bilingual Evaluation Understudy): mede quanto o texto do modelo reproduz n‚Äëgramas presentes no gabarito. Calculamos BLEU‚Äë4 via sacrebleu.corpus_bleu, ou seja, observamos de 1 a 4 palavras consecutivas. Ele produz um score de 0 a 100 (100 = texto id√™ntico). Valores altos indicam que a resposta bate nos mesmos trechos e sequ√™ncias; baixos significam vocabul√°rio/estrutura bem diferentes. √â uma m√©trica mais r√≠gida, focada em sobreposi√ß√£o literal.

ROUGE‚ÄëL: compara subsequ√™ncias comuns mais longas (Longest Common Subsequence). Usamos rouge_score.RougeScorer(["rougeL"]) e o resultado tamb√©m vai de 0 a 100 porque multiplicamos por 100. ROUGE‚ÄëL captura ordem de frases e cobertura global; funciona melhor que BLEU quando o modelo reescreve com outras palavras mas mant√©m a estrutura principal.

Resumindo: BLEU avalia precis√£o literal dos n‚Äëgramas; ROUGE‚ÄëL mede cobertura estrutural (recall). Ambos olham texto inteiro sem truncar, complementando o BERTScore (que √© sem√¢ntico).
Hoje n√£o configuramos nenhum ‚Äúpercentual m√≠nimo de compatibilidade‚Äù (limiar de similaridade) nos vetores. Todos os backends retornam os top_k resultados sem filtragem adicional; a ordena√ß√£o j√° vem pela m√©trica de dist√¢ncia (cosine). Se quiser descartar trechos abaixo de algum score, precisaria p√≥s-processar a lista retornada (por exemplo, ignorar itens com similaridade <0.3 usando o vector_debug).

O par√¢metro top_k fica hardcoded em main.py, fun√ß√£o query_backend. Cada chamada usa top_k=3 (antes da sua altera√ß√£o). Se voc√™ mudar para 5, a API e a UI v√£o recuperar cinco chunks em vez de tr√™s, o que tende a dar mais contexto ao LLM. Isso pode melhorar respostas quando o manual tem trechos espalhados, mas tamb√©m aumenta o prompt e custo (tokens) ‚Äî vale observar se o modelo come√ßa a se dispersar ou estourar o limite.

No runner offline (industrial_dual_rag.py), o retrieve_context usa top_k=3 tanto para Chroma quanto para o backend ‚Äúsimple‚Äù. Se quiser comparar os cen√°rios com o mesmo top_k=5, ajuste l√° tamb√©m.

Resumindo: n√£o temos threshold; s√≥ controlamos quantos chunks entram (top_k). Ajustar para 5 √© uma boa hip√≥tese quando voc√™ precisa de mais cobertura, desde que monitore o tamanho do prompt e mantenha a coer√™ncia com os gabaritos.