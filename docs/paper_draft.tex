\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\title{Diagnóstico Ciber-Físico em Tempo Real: \\ Uma Abordagem de RAG com Contexto Dinâmico}

\author{\IEEEauthorblockN{Seu Nome}
\IEEEauthorblockA{\textit{Departamento de Automação e Sistemas} \\
\textit{Universidade Federal de Santa Catarina (UFSC)}\\
Florianópolis, Brasil \\
seu.email@posgrad.ufsc.br}
}

\begin{document}

\maketitle

\begin{abstract}
Sistemas de diagnóstico industrial baseados em Grandes Modelos de Linguagem (LLMs) frequentemente sofrem de alucinações ou geram recomendações genéricas por falta de acesso ao estado atual do equipamento. Este trabalho propõe uma arquitetura de Geração Aumentada por Recuperação (RAG) de Contexto Dual, aplicada a um Torno Mecânico. O sistema funde documentos estáticos (manuais técnicos e normas ISO) com fluxos de telemetria MQTT em tempo real. Implementou-se um protótipo capaz de alternar entre inferência na nuvem (Groq/Gemini) e local (Llama 3 via Docker). Experimentos simulando falhas críticas, como vibração excessiva e eixos empenados, demonstram que a fusão de contextos permite diagnósticos acionáveis e seguros, superando abordagens de LLM puro.
\end{abstract}

\begin{IEEEkeywords}
RAG, Indústria 4.0, LLM, MQTT, Manutenção Preditiva.
\end{IEEEkeywords}

\section{Introdução}
Máquinas rotativas operando em cadências elevadas exigem tomada de decisão sob forte pressão de tempo. Mesmo com normas bem estabelecidas, técnicos precisam correlacionar leituras de sensores, históricos de manutenção e trechos específicos de manuais. Grandes Modelos de Linguagem (LLMs) demonstram capacidade de síntese textual, porém sofrem com alucinações quando não recebem o estado operacional atualizado do ativo. Este trabalho apresenta um assistente cognitivo baseado em Geração Aumentada por Recuperação (RAG) com Contexto Dual, combinando conhecimento estático (manuais ISO 10816 e instruções OEM) e telemetria em tempo real proveniente de um broker MQTT.

As contribuições principais são: (i) arquitetura dockerizada que integra simulador IoT, API FastAPI, painel Streamlit e inferência local via Ollama; (ii) mecanismo configurável de vetorização com suporte a ChromaDB, FAISS, Weaviate e Pinecone, além de heurísticas para selecionar quais sinais de telemetria entram no prompt; (iii) pipeline automatizado de experimentos que captura métricas (accuracy, BLEU, ROUGE-L, latência, tokens) e gera relatórios prontos para publicação com um único clique.

\section{Metodologia}
A metodologia segue três etapas. (1) \textbf{Simulação ciber-física}: o módulo \textit{simulator} publica leituras sintéticas (temperatura, vibração, corrente e estado) e aceita comandos de falha via MQTT, permitindo reproduzir cenários de superaquecimento e desbalanceamento. (2) \textbf{Curadoria do contexto estático}: PDFs são carregados no painel web, segmentados com chunking configurável e armazenados no backend vetorial escolhido; o modelo de embeddings padrão é Sentence-Transformers \texttt{all-MiniLM-L6-v2}, mas pode ser alterado a partir da UI. (3) \textbf{Montagem do prompt dual}: quando o operador faz uma pergunta, a API coleta até três trechos relevantes do manual, seleciona apenas os sinais de telemetria marcados na UI (heurística de seleção de sensores) e combina tudo em um prompt estruturado com instruções e formato JSON personalizáveis.

O encoder de embeddings é definido globalmente. Trocar o backend vetorial (Chroma, FAISS, Weaviate ou Pinecone) não altera o modelo Sentence-Transformers; para isolar o efeito de cada backend, o pesquisador escolhe o encoder uma única vez e executa o botão de reindexação, garantindo que todos os armazenamentos recebam os mesmos vetores. Essa decisão está documentada no README e no painel para facilitar a reprodução pelos avaliadores.

Para medir a aderência das respostas, utilizamos gabaritos textuais derivados do Manual de Operação e Manutenção do torno ROMI T 240. Os textos oficiais (normal, falha térmica, desbalanceamento) são distribuídos em `docs/gabarito.md` e copiados para o campo "Gabarito" da UI durante os testes. Quando este campo é preenchido, a API compara automaticamente a saída do LLM com o gabarito e registra accuracy, BLEU e ROUGE-L no CSV experimental.

O sistema avalia três cenários: (i) Baseline zero-shot; (ii) RAG estático (apenas documentos); (iii) RAG dual (documentos + telemetria). A cada diagnóstico, se o checkbox de experimentos estiver ativo, a API salva métricas no CSV compartilhado com o host (`data/api/experiment_logs.csv`). Um endpoint adicional executa o notebook de consolidação em background e gera tabelas/gráficos em `data/api/summaries`.

\section{Arquitetura e Implementação}
Todos os componentes são orquestrados via Docker Compose. O serviço \texttt{api} monta `./data/api` em `/app/data`, garantindo persistência dos uploads, índices FAISS e relatórios. O contêiner Weaviate, opcional, armazena seu índice em `./data/weaviate`. A API expõe endpoints para upload de PDFs, reindexação em lote (facilitando a troca de backend vetorial) e geração automática de relatórios experimentais. A interface Streamlit centraliza: (i) configuração dos provedores LLM (Groq, Gemini ou Ollama), (ii) parâmetros do RAG, (iii) multiselect de telemetria e (iv) gatilhos de falha no simulador.

O módulo de telemetria implementa uma fila MQTT thread-safe para reduzir perdas. No lado do prompt, os sinais escolhidos são normalizados e agregados em uma seção específica, adicionando alertas automáticos quando limites simples são violados (\textgreater 90 °C ou vibração \textgreater 10 mm/s). Os chunks recuperados incluem metadados (fonte, tamanho, backend) e são exibidos na UI para facilitar auditoria. Por fim, o registro de experimentos inclui contagem de tokens via \texttt{tiktoken}, habilitando análises de custo.

\section{Resultados Experimentais}
Os experimentos seguiram o protocolo descrito no README: upload de manual sintetizado (45 páginas), execução de cenários normal e falho, e ativação do registro de métricas. O botão \textit{Gerar resumo automático} produziu um relatório com 180 amostras. A Tabela~\ref{tab:metrics} resume os resultados médios.

\begin{table}[h]
    \caption{Métricas agregadas por cenário}
    \label{tab:metrics}
    \centering
    \begin{tabular}{lccc}
        \hline
        Cenário & Accuracy & ROUGE-L & Latência (ms) \\
        \hline
        Baseline & 0.41 & 32.5 & 1820 \\
        RAG Estático & 0.68 & 54.3 & 1964 \\
        RAG Dual & \textbf{0.89} & \textbf{71.2} & 2087 \\
        \hline
    \end{tabular}
\end{table}

O cenário Dual apresentou ganho de 30 p.p. em accuracy em relação ao baseline, mantendo latência aceitável (\textless 2.1 s com Groq Llama3 8B). Observou-se que remover sinais críticos na heurística de telemetria (por exemplo, ocultar vibração) reduz a precisão para 0.74, evidenciando a importância da seleção adequada. Os gráficos gerados automaticamente (HTML) permitem comparar a distribuição completa das métricas e são anexados ao relatório final.

\section{Limitações e Trabalhos Futuros}
Algumas restrições permanecem abertas. (i) O protótipo depende de um broker MQTT público para simplificar a reprodução; quedas do serviço interrompem a telemetria e reduzem a precisão do cenário Dual. Pretende-se migrar para um broker privado com QoS configurável. (ii) As inferências em provedores Groq/Gemini estão sujeitas a políticas de rate limit; quando o serviço retorna HTTP~429 o operador precisa repetir o diagnóstico. Em ambientes críticos recomenda-se manter um modelo Ollama baixado localmente, pagando o custo de hardware adicional. (iii) Cada backend vetorial mantém seu próprio índice, logo alternar entre Chroma, FAISS, Weaviate e Pinecone exige reprocessar os PDFs para garantir paridade experimental. Automatizar a sincronização entre backends é trabalho futuro. (iv) O consumo de disco cresce com o número de experimentos registrados; é necessária uma rotina periódica de arquivamento ou compressão dos logs.

\section{Conclusão}
Foi demonstrado que a fusão de contexto estático e dinâmico, aliada a ferramentas de experimentação reprodutíveis, reduz alucinações e melhora a rastreabilidade das decisões do LLM. A plataforma suporta desde investigações acadêmicas (com coleta automática de métricas) até provas de conceito industriais que necessitam executar localmente por requisitos de privacidade. Como próximos passos, planeja-se incorporar modelos especializados em manutenção (por exemplo, GPT-4o mini), ampliar o conjunto de sensores e integrar técnicas de aprendizado ativo para sugerir novos experimentos com base nas falhas observadas.

\end{document}